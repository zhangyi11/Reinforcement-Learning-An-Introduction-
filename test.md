公式2.3的更新规则是本书中最常出现的更新规则，一般形式为

$$新估计值 \leftarrow 旧估计值+ 步长[目标-旧估计值]$$

表达式$`[目标-旧估计值]`$是估计的误差。通过向‘目标’迈进以减小误差。目标指示了一个理想的前进方向，尽管目标可能带有噪声。在老虎机问题中目标是第n次获得的奖励。

我们注意到，在公式2.3增量方法中使用的步长参数会随着时间步长的变化而变化。处理动作a第n次奖励时，增量方法使用的步长参数为$`\frac{1}{n}`$。本书中我们用$`\alpha`$或$`\alpha_t(a)`$来表示步长参数。

以下伪代码使用了样本平均法，$`\epsilon`$-贪婪方法和增量公式表示老虎机问题的算法，函数bandit(a)返回动作a相应的奖励。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/A%20simple%20bandit%20algorithm.jpg)

## 2.5 追踪非平稳问题
到目前为止讨论的求平均值方法都是针对稳态老虎机问题，即奖励不随时间的变化而变化。如前所述，我们接触的大多数强化学习问题都是非稳态的。在非稳态的强化学习问题中，短期内获得奖励的权重高于长期的奖励权重是有意义的。最常用的方法就是使用一个固定的步长参数。例如在增量公式2.3中，用前n-1次奖励更新平均值$`Q_n`$的公式可以调整为

$$Q_{n+1}\dot{=}Q_n+\alpha[R_n-Q_n]$$

其中$`\alpha\in(0,1]`$。公式表明$`Q_{n+1}`$是初始估计值$`Q_1`$和过去奖励和的加权平均。

$$ Q_{n+1} = Q_n+\alpha[R_n-Q_n] = \alpha R_n+(1-\alpha)Q_n = \alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]=(1-\alpha)^nQ_1+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-1}R_i$$

$$ 其中，(1-\alpha)^n + \sum_{i=1}^{n}\alpha(1-\alpha)^{n-1} = 1，即权重之和为1 $$

权重$`\alpha(1-\alpha)^{n-1}`$取决于$`R_i`$之间有多少奖励，$`1-\alpha`$的值小于1，介于两个奖励之间的奖励数量越多，$`R_i`$的权重就越小。实际上，权重根据$`1-\alpha`$上的指数呈指数衰减（如果$`1-\alpha=0`$，那么所有的权重都放在最后的奖励$`R_n`$上，因为$`0^0`=1$）。

$`\alpha_n(a)`$表示用于处理第n次选择行动a获得的奖励参数。如前所述，样本平均法的步长参数$`\alpha_n(a)=\frac{1}{n}`$，大数定律保证了该方法可以收敛到真实的行动价值。但是无法保证序列$`{\alpha_n(a)}`$的所有选择都能收敛。随机近似理论中的著名结论确保了与概率1收敛所需要的条件

$$\sum_{n=1}^{\infty}\alpha_n(a)=\infty \ \ 和\ \  \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty$$
