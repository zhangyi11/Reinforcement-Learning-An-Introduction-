## 2.6 乐观初始值
我们讨论过的所有方法在一定程度上都依赖于初始值$`Q_1(a)`$，用统计学的语言来说，这些方法都会受到初始值的偏差影响。对于样本平均法来说，一旦所有动作都被选择过一次，偏差就会消失；但对于固定步长参数$`\alpha`$来说，偏差永久存在，尽管偏差会随着步长的增大而逐渐减小如2.6公式所示。在实践中，这些偏差并不是什么问题，有时甚至对我们非常有帮助。缺点是，初始估计变成了一组需要用户选择的参数，即使将初始值全部设为0。好的一面是，设置初始值是可以看成获得预期奖励的鲜艳知识。

初始化行动价值也可用作鼓励探索，比如我们将10臂老虎机的初始估计价值设置为5，而不是0（10臂老虎机行动的真实价值是从均值为0方差为1的正态分布中随机抽取的）。因此奖励值正5是非常乐观的，这种乐观的初始值鼓励了探索，即无论最初选择哪个动作，获得的奖励都会低于5，算法就会选择其他的行动（译者注：因为其他行动的估计价值为+5，智能体会选择奖励值高的行动作为实际行动）。由于获得的奖励远低于初始估计值，智能体会感到‘失望’，结果在估计价值收敛前，所有行动都会尝试多次，即使一直选择贪婪行动，智能体仍会有相当数量的探索行动。

图2.3展示了使用贪婪方法，初始估计值为正5的10臂老虎机的性能，与使用$`\epsilon`$-贪婪方法，初始估计值为0作比较。最初乐观初始值性能较差，因为在探索上花费了大量的时间，但最终乐观初始值变现出的性能优于$`\epsilon`$-贪婪方法，因为随着步数的增多，探索的次数逐步降低。我们把这个鼓励探索的技术称为乐观初始值。我们认为这是一种在平稳问题上相当有效的技巧，但远未成为一种普遍有用的探索方法。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/figure-2.3.png)
**图2.3** 10臂老虎机上使用乐观初始值的效果，两种方法都用了固定步长参数，$`\alpha=0.1`$

乐观初始值并不适用于非平稳问题，因为它探索的驱动力本质是暂时的。如果问题发生变化，需要重新探索，这种方法就无能为力了。事实上，聚焦在初始值上的任何方式方法都不太能帮助解决非平稳的强化学习问题。初始值只在最开始时出现一次，因此我们没有必要过分关注它。同样的问题也出现在样本平均法上，样本平均法同样将时间的起点当做一个特殊事件，然后对所有后续的奖励等权平均。不过，这些方法都非常简单，其中之一或者它们之中某种的简单组合在实际中通常就足够了，在本书的剩余部分，我们经常使用其中集中简单的探索方法。

_练习2.6_：神秘的尖峰 在图2.3中的结果应该相当可靠，因为它们是对2000多个随机选择10臂老虎机的平均值，为什么乐观初始值方法的曲线会有波动和尖峰呢？换句话说，是什么可能导致这种方法在特定的早起步骤中平均变现的特别好或特别差。

_答案_ :由于初始化的Q值比较大，若真实Q值比初始值小，那么前期每访问一个动作，其Q值会逐渐被更新小。从而每个动作都会被访问到，这也导致前期策略的探索性很大。一段时间更新之后，Q值的估计逐渐变小变准确，从而最终大概率选到最优动作。所以整个过程前期由于Optimistic Initial Values的存在，动作的选择探索性较高，会出现震荡，随后Q的估计逐渐变准，初始值的影响变小，Q值逐渐收敛。影响效果的因素和初始化的Q值，以及真实的Q值有关。由于没有ϵ探索策略，若真实的值比初始值大，可能选到一个动作之后，就再也跳不出局部最优，此时曲线不会震荡，但表现一直维持在很低的水平不会提升。相反若真实的值比初始值小，则会出现图2.3的情形。若真实的值和初始值的差异不大，此时的效果可能会比较平稳，而算法的探索性更多可能会取决于奖励的方差。


