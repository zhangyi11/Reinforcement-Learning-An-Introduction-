## 2.8 老虎机梯度算法
目前为止，我们研究了估计行动价值的方法并使用这些估计价值来选择行动。通常来说，这是一个好方法，但不是唯一可行的方法。在本节，我们考虑学习每一个行动a的数字偏好，用$`H_t(a)\in\mathcal{R}`$来表示。偏好越大，采取行动的频率也就越高，但这种偏好在奖励方面没有解释。重要的是一个动作和其他动作之间的相对偏好；如果我们对所有动作的偏好值都增加一个常数，比如1000，这对动作选择的概率没有影响。这是因为动作选择的概率是根据软最大分布（如吉布斯分布或玻尔兹曼分布）计算的，而这种分布只取决于各动作偏好值之间的相对差异，而不是它们的绝对值。

$$ Pr(A_t = a) \doteq \frac{e^{H_t(a)}}{\sum{^{k}_{b=1} e^{H_t(b)}}} \doteq \pi_t(a) \ \ (2.11)$$

我们引入了一个新的表示法$`\pi_t(a)`$，表示在t时刻采取行动a的概率。所有行动的初始偏好值为0（$`H_1(a)=0`$）,这样所有行动在一开始被选择到的概率相同.

_练习2.9_ 证明，在只有两个动作的情况下，软最大分布与统计学和人工神经网络中常用的逻辑函数，如sigmoid函数，给出的分布相同。

基于随机梯度上升的思想，对于软最大值行动偏好，存在一种自然学习算法。在选择行动$`A_t`$和获得奖励$`R_t`$后，行动偏好值的更新公式

$$ H_{t+1}(A_t) \dot{=} H_t(A_t) + \alpha(R_t-\bar{R_t})(1-\pi_t(A_t)) \ \ (2.12) $$

$$ H_{t+1}(a) \dot{=} H_t(a) - \alpha(R_t-\bar{R_t})(1-\pi_t(a))  \ \ (a \neq A_t)\ \  (2.12)$$

其中$`\alpha`$是大于0的步长参数，$`\bar{R_t} \in \mathbb{R}`$是获得奖励的平均值，不包括时刻t的奖励$`(\bar{R_1} \dot{=} R_1)`$，可以用节2.4或节2.5中的增量公式计算。$`\bar{R_t}`$提供了一个用于与当前奖励比较的基线。如果获得的奖励高于基线，那么在未来采取行动$`A_t`$的概率会增加，其他行动被选择的概率就会减少，如果获得的奖励低于基线，采取行动$`A_t`$的概率会减少，其他行动被选择的概率就会增加。

图2.5展示了在10臂老虎机上使用梯度算法的结果，其中真实期望奖励是按照均值为+4（而不是0）的正态分布选择的（方差仍为1）。所有奖励的上移对梯度老虎机算法没有任何影响，因为奖励基线能够即时适应新的水平。但如果删除基线，将公式2.12中$`\bar{R_t}`$的值恒定为0，则性能会有显著下降，如图所示。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/figure-2.5.png)
**图2.5** 老虎机梯度算法在10臂老虎机上的平均性能（带有和不带有奖励基线），$`q_*(a)`$接近+4而不是0。
