## 2.6 乐观初始值
我们讨论过的所有方法在一定程度上都依赖于初始值$`Q_1(a)`$，用统计学的语言来说，这些方法都会受到初始值的偏差影响。对于样本平均法来说，一旦所有动作都被选择过一次，偏差就会消失；但对于固定步长参数$`\alpha`$来说，偏差则永久存在，尽管偏差会随着步长的增大而逐渐减小如2.6公式所示。在实践中，这些偏差并不是什么问题，有时甚至对我们非常有帮助。缺点是，初始值变成了一组需要用户选择的参数。即使将初始值全部设为0。好的一面是，初始值是可以看成获得预期奖励的先验知识。

初始化行动价值也可鼓励探索，比如我们将10臂老虎机的初始估计价值设置为5，而不是0（10臂老虎机行动的真实价值是从均值为0方差为1的正态分布中随机抽取的）。因此奖励值正5是非常乐观的，这种乐观的初始值鼓励了探索，即无论最初选择哪个动作，获得的奖励都会低于5，算法就会选择其他的行动（译者注：因为其他行动的估计价值为+5，智能体会选择奖励值高的行动作为实际行动）。由于获得的奖励远低于初始估计值，智能体会感到‘失望’，因此在估计价值收敛前，所有行动都会尝试多次，即使一直选择贪婪行动，智能体仍会有相当数量的探索行动。

图2.3展示了使用贪婪方法，初始估计值为正5的10臂老虎机的性能，与使用$`\epsilon`$-贪婪方法，初始估计值为0作比较。最初乐观初始值性能较差，因为在探索上花费了大量的时间，但最终乐观初始值变现出的性能优于$`\epsilon`$-贪婪方法，因为随着步数的增多，探索的次数逐步降低。我们把这个鼓励探索的技术称为乐观初始值。我们认为这是一种在平稳问题上相当有效的技巧，但远未成为一种普遍有用的探索方法。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/figure-2.3.png)
**图2.3** 10臂老虎机上使用乐观初始值的效果，两种方法都用了固定步长参数，$`\alpha=0.1`$

乐观初始值并不适用于非平稳问题，因为它探索的驱动力本质是暂时的。如果问题发生变化，需要重新探索，这种方法就无能为力了。事实上，聚焦在初始值上的任何方式方法都不太能帮助解决非平稳的强化学习问题。初始值只在最开始时出现一次，因此我们没有必要过分关注它。同样的问题也出现在样本平均法上，样本平均法同样将时间的起点当做一个特殊事件，然后对所有后续的奖励等权平均。不过，这些方法都非常简单，其中之一或者它们之中某种的简单组合在实际中通常就足够了，在本书的剩余部分，我们经常使用其中几种简单的探索方法。

_练习2.6_：神秘的尖峰 在图2.3中的结果应该相当可靠，因为它们是对2000多个随机选择10臂老虎机的平均值，为什么乐观初始值方法的曲线会有波动和尖峰呢？换句话说，是什么可能导致这种方法在特定的早起步骤中平均变现的特别好或特别差。

_答案_ :由于初始化的Q值比较大，若真实Q值比初始值小，那么前期每访问一个动作，其Q值会逐渐被更新小。从而每个动作都会被访问到，这也导致前期策略的探索性很大。一段时间更新之后，Q值的估计逐渐变小变准确，从而最终大概率选到最优动作。所以整个过程前期由于乐观初始值的存在，动作的选择探索性较高，会出现震荡，随后Q的估计逐渐变准，初始值的影响变小，Q值逐渐收敛。影响效果的因素和初始化的Q值，以及真实的Q值有关。由于没有ϵ探索策略，若真实值比初始值大，可能选到一个动作之后，就再也跳不出局部最优，此时曲线不会震荡，但表现一直维持在很低的水平不会提升。相反若真实的值比初始值小，则会出现图2.3的情形。若真实的值和初始值的差异不大，此时的效果可能会比较平稳，而算法的探索性更多可能会取决于奖励的方差。

_练习2.7_：无偏固定步长技巧 本章的大部分内容中，我们使用样本平均来计算行动的估计价值，因为样本平均法不会和固定步长参数一样产生初始偏差。然而，样本平均值并不是一个完全令人满意的解决方案，因为它在非平稳性问题上表现不佳。是否有可能避免固定步长带来的偏差，同时保留它们在非平稳问题上的优势？一种方法是使用步长参数，来处理某行动第n次奖励。

$$ \beta_n\dot{=}\alpha/\bar{o}_n \ \ (2.8)\ $$

$`\ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \  \bar{o}_n\dot{=}\bar{o}_{n-1}+\alpha(1-\bar{o}_{n-1}),n>0,\bar{o}_0 \ \dot{=}0 \ \ (2.9) `$ 

其中$`\alpha`$是一个大于0的固定步长参数，进行类似于 (2.6) 的分析，证明$`Q_n`$是没有初始偏差的指数加权平均值。

## 2.7 置信上限行动选择
探索是必要的，因为行动估计价值的准确性总是不确定的，贪婪行动是目前看起来最好的选择，但其他的行动实际上可能更好，$`\epsilon`$-贪婪方法迫使选择非贪婪方法，但选择非贪婪行动时是纯随机没有任何偏好，我们最好根据行动的潜力选择非贪婪方法，考虑估计价值和最大价值的不确定性和估计的不确定性。一种有效的方式是根据

$$ A_t\ \dot{=}\ \underset{a}{argmax}[Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}] \ \ (2.10)$$

$`N_t(a)`$表示在时刻t前选择行动a的次数，c的数值大于0控制探索的程度，如果$`N_t(a)=0`$，那么a就是最大化奖励的行动。

这种置信上限的行动选择思想是，平方根项是行动a估计价值的不定性或方差的度量。用此公式计算出来的数值是行动a可能真实值的上限，c确定了置信水平。每此选择动作a时，不确定性都会降低：$`N_t(a)`$增加，并且出现在分母上。另一方面，每次选择a以外的行动时，t都会增加但是$`N_t(a)`$不变，因此不确定性会增加。使用自然对数意味着，随着时间的变化，增加值会不断的变小，但也不会受到限制。最终，所有的行动都会被选择，但对有较低估计价值的行动，或者经常被选择的行动，会随着时间的推移而被选择的频率逐渐降低。

图2.4表显示，在10臂老虎机上使用置信上限行动选择方法。相比于$`\epsilon`$-贪婪方法，置信上限行动选择法通常表现的更好，但将其扩展到老虎机问题以外的强化学习问题较为困难。其中一个难题是，在处理非平稳强化学习问题时，需要比第 2.5 节中介绍的方法更复杂的方法。另一个问题是在处理大型状态空间时，特别是在使用本书第二部分介绍的函数逼近方法时。在这些更高级的环境中，置信上限行动选择方法并不实用。

