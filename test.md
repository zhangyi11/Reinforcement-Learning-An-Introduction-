## 2.6 乐观初始值
我们讨论过的所有方法在一定程度上都依赖于初始值$`Q_1(a)`$，用统计学的语言来说，这些方法都会受到初始值的偏差影响。对于样本平均法来说，一旦所有动作都被选择过一次，偏差就会消失；但对于固定步长参数$`\alpha`$来说，偏差永久存在，尽管偏差会随着步长的增大而逐渐减小如2.6公式所示。在实践中，这些偏差并不是什么问题，有时甚至对我们非常有帮助。缺点是，初始估计变成了一组需要用户选择的参数，即使将初始值全部设为0。好的一面是，设置初始值是可以看成获得预期奖励的鲜艳知识。

初始化行动价值也可用作鼓励探索，比如我们将10臂老虎机的初始估计价值设置为5，而不是0（10臂老虎机行动的真实价值是从均值为0方差为1的正态分布中随机抽取的）。因此奖励值正5是非常乐观的，这种乐观的初始值鼓励了探索，即无论最初选择哪个动作，获得的奖励都会低于5，算法就会选择其他的行动（译者注：因为其他行动的估计价值为+5，智能体会选择奖励值高的行动作为实际行动）。由于获得的奖励远低于初始估计值，智能体会感到‘失望’，结果在估计价值收敛前，所有行动都会尝试多次，即使一直选择贪婪行动，智能体仍会有相当数量的探索行动。

图2.3展示了使用贪婪方法，初始估计值为正5的10臂老虎机的性能，与使用$`\epsilon`$-贪婪方法，初始估计值为0作比较。最初乐观初始值性能较差，因为在探索上花费了大量的时间，但最终乐观初始值变现出的性能优于$`\epsilon`$-贪婪方法，因为随着步数的增多，探索的次数逐步降低。我们把这个鼓励探索的技术称为乐观初始值。我们认为这是一种在静态问题上相当有效的技巧，但远未成为一种普遍有用的探索方法。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/figure-2.3.png)
**图2.3** 10臂老虎机上使用乐观初始值的效果，两种方法都用了固定步长参数，$`\alpha=0.1`$
