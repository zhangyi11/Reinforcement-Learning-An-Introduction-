# Chapter2 多臂老虎机
强化学习与其他类型学习最重要的区别在于，强化学习利用训练信息来评估所采取的行动，而不是给出正确的行动来指导。这就产生了积极探索的需求和搜索好行为的需求。纯粹的评估性反馈能指出行动的好坏程度，但无法表明该行动是最好或最差的行动。另一方面，纯粹的指导性反馈表明了要采取的正确行动，独立于实际采取的行动。指导性反馈是监督学习的基础，涵盖了模式分类、系统识别、人工神经网络的大部分内容。这两种反馈是截然不同的，评估反馈完全依赖于所采取的行动，而指导性反馈则独立于所采取的行动。

在本章中，我们在一个简化的环境中研究强化学习的评估方面，这个环境只涉及在单一情境中的学习。这种非关联性的环境是大多数之前关于评估反馈的研究所采用的，它避免了完整强化学习问题的复杂性 **（译者注：之前的研究指的是在强化学习领域中，以非关联性环境为基础做的研究工作，这些研究主要关注评估反馈如何影响学习过程，而不涉及在不同情境中选择不同最佳行动的复杂性）**。研究这种情况使我们能够更清楚地理解评估反馈与指导性反馈之间的区别，同时也能看到它们如何结合使用。

我们探讨的特定非关联性，评估反馈问题是k臂老虎机问题的简化版。我们用这个问题介绍一些学习的基本方法，并在之后的章节逐步扩展这些方法，应用于完整的强化学习问题。在本章的结尾，我们将进一步接近完整的强化学习，讨论当老虎机问题变成关联性问题时会发生什么，也就是说，当最佳行动取决于具体情境时会如何。


##  2.1 k臂老虎机问题
考虑如下学习问题，你需要不断地在k个不同的选项中做出选择，在每次选择后都会得到数值奖励，该数值奖励服从稳态概率分布，该分布取决与你的选择 **（译者注：稳态概率分布是指在一个随机过程或马尔可夫链中，随着时间推移，系统状态的概率分布趋于稳定，不再随时间变化。）** 你的目标是在某个时间段内最大化预期总奖励，例如，1000个行动选择或1000个时间步长。

这就是k臂老虎机问题的原始形式，因类比于老虎机而得名，只不过k臂老虎机有k个拉杆而不是一个。每次选择就像是拉动k臂老虎机的一个拉杆，奖励就是击中头奖的回报。在重复的拉动中，你将集中拉动k个拉杆之中的一个，以获得最大的奖励。

在我们的k臂老虎机问题中，每个拉杆被选择时，都会有一个期望或平均奖励，我们将平均奖励称为该行动的价值。我们用A<sub>t</sub>表示在时间步t做的选择，选择A<sub>t</sub>的奖励用R<sub>t</sub>表示。任意操作a的价值用$`q_{\ast}(a)`$表示，$`q_{\ast}(a)`$是行动a的预期奖励：
$q_{\ast}(a) \dot{=} \mathbb{E}[R_t \mid A_t = a]$

如何你知道每个行动的价值，那么解决k臂老虎机问题将会变的非常容易：做选择时，总是拉动具有最高价值的拉杆。假设你不确定每个行动的价值，但你知道每个行动的估计价值，我们将时间步t行动a的估计价值记为Q<sub>t</sub>(a)。我们希望Q<sub>t</sub>(a)无限接近$`q_{\ast}(a)`$。

如果你持续的更新每个行动的估计价值，那么在任意时间步做选择时，至少有一个行动的估计价值是最大的。我们称之为贪婪行动 **（译者注：贪婪行动，即每次做选择时都选择估计价值最大的行动为实际行动）** 当您选择估计值最大的行动作为实际行动的时候，我们说你正在**利用**当前行动价值的知识。如果你选择一个非贪婪行动，那我们说你正在**探索**，探索可以帮助你改进非贪婪行动的估计价值。在某一时间步，为了最大化预期的奖励，利用（即选择贪婪行动）是正确的做法，但从长远来看，探索可能会产生更大的总奖励。

举个例子，假设贪婪行动的价值是确定的，而与贪婪行动的价值相比，其他几个行动的估计价值相当或相近。但是非贪婪行动伴随着很大的不确定性。这种不确定性意味着，尽管贪婪行动的价值看似最优，但实际上至少有一个非贪婪行动的价值比贪婪行动的价值高，只是我们无法确切知道是哪一个非贪婪行动。如果你在未来有很多时间步做选择，那么最好探索非贪婪行动，发现非贪婪行动中哪一个行动的价值高于贪婪行动。

从短期来看，探索带来的奖励较低，但从长期来看，探索带来的奖励更高，因为一旦你发现了比贪婪行动更好的行动，你就可以利用它很多次。由于无法在一次选择中兼顾探索和利用，因此探索和利用之间存在“冲突”。

做选择时，探索和利用哪个更好，取决于估计价值的精确度、不确定性以及剩余步数等复杂因素。对于k臂老虎机等相关特定问题的数学模型，有许多复杂的方法平衡探索与利用。然而，这些方法中的大多数都对平稳性和先验知识做出了强有力的假设，这些假设在大多数应用中、以及在我们后续章节的完整强化学习问题中要么不成立，要么无法验证。当理论假设不适用时，这些方法所提供的最优性或有界损失对我们没有实质性的帮助。

本书不会用复杂的方式详细讨论如何平衡探索和利用，我们只关心如何在整体上平衡探索和利用。本章会介绍几种简单的平衡方法来解决k臂老虎机问题，平衡探索和利用的方法比总是利用的方法效果要好得多。平衡探索和利用是强化学习中独具特色的挑战，k臂老虎机能够清晰的展示这一点。

## 2.2 行动价值方法
首先，我们研究行动的估计价值函数，并用行动的估计价值做出决策。回想一下，行动的真实价值是选择该行动时会得到的平均奖励。因此，我们自然的会想到用行动曾经获得奖励的平均值作为该行动的估计价值:
$Q_t(a)\dot{=}\frac{时间步t之前行动a获得奖励的总和}{时间步t之前行动a被选择的次数}=\sum\limits_{i=1}^{t-1}R_i \cdot \mathbf{1}_{\{A_i = a\}}$
公式中$`1_{predicate}`$指，当predicate为真时$`1_{predicate}`$的值为1，当predicate为假时，$`1_{predicate}`$的值为0。如果分母为0，我们将$`Q_t(a)`$定义为某个默认值，例如0。当分母趋于无穷时，根据大数定律$`Q_t(a)`$收敛于$`q_*(a)`$，我们此公式来估计行动的价值，由于每个估计值都是样本相关奖励的平均值，故称此方法为样本平均法。样本平均法只是估计行动价值的一种方法，不一定是最好的方法。现在，我们来探讨如何使用样本平均法计算出的估计值来选择动作。

最简单的选择动作规则就是选择具有最高估计价值的行动，即上一节定义的贪婪操作。如果有多个贪婪的行动（译者注：有多个相等的最大估计价值），那么就会以任意方式挑选一个，也许是随机挑选。我们将贪婪的行动选择方法写成
$A_t\dot{=}\underset{a}{argmax}Q_t(a)$公式中
$\underset{a}{argmax}$表示的含义是使得$`Q_t(a)`$最大化的动作a（有多个最大值时任意选择其中一个动作）。贪婪的行动选择总会利用已知信息最大化即时奖励，不会花时间去选择那些明显奖励低的动作。一个简单的替代方法就是，大多时候采取贪婪策略，每隔一段时间（以一个小概率𝜖），从所有行动中随机选取一个行动，此时所有动作被选择的概率相等，与行动估计价值无关。我们将这种近似贪婪的行动选择策略称为𝜖-贪婪方法。

𝜖-贪婪方法的优势在于，随着步数的增加，每个行动都会被采样无数次，从而确保了所有行动$`Q_t(a)`$都收敛于各自的$`q_*(a)`$。这意味着选择最优行动的概率大于1-𝜖，即接近确定性。虽然这些方法在理论上提供了渐近性保证（译者注：渐进性保证的含义是，某种性质在某种极限条件如样本数量无穷大、时间步长无穷大时得到保证），但这种保证并不能保证这些方法在实际应用中的有效。

_练习2.1_  使用𝜖-贪婪方法选择行动，当可选择的行动数量为2，𝜖=0.5时，贪婪动作被选中的概率是多少？

_答案_ 0.75 （译者注：0.5+0.5*1/2=0.75，其中1为贪婪行动的动作数量，2为可选择的行动数量）

## 2.3 10臂老虎机
为了粗略的评估贪婪和𝜖-贪婪行动价值方法的相对效率，我们在一组测试问题上对两种方法在数值上进行了比较。这是一组包含2000个随机生成的k臂老虎机问题，其中k=10。老虎机的每个拉杆都有相应的行动价值，记为$`q_*(a)`$，a=1,...,10，如图2.1。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/figure-2.1.png)
**Figure2.1**：一个10臂老虎机的例子，每个行动的真实值$`q_*(a)`$都是根据均值为0、方差为1的正态分布选择的，每个行动的实际奖励值服从均值为$`q_*(a)`$，方差为1的正态分布，如图中灰色所示。

当某种学习方法应用于该问题时，如在时间步长t选择动作$`A_t`$,该动作的实际奖励$`R_t`$选择自均值为$`q_*(A_t)`$方差为1的正态分布中。图2.1中灰色的部分表示奖励分布。我们称这一组测试为10臂老虎机。使用任和学习方法，当其应用于老虎机问题时，我们可以在1000个时间步来测量其随着经验增加而改进的行为。这就构成了一次运行。重复运行上述操作2000次，每一次都运行在不同的臂上，我们便获得了学习算法平均行为的度量。（译者注：对于10臂老虎机来说，重复运行的操作次数为2000次，平均下来老虎机的每一个臂重复运行2000/10=200次，每一次运行1000个时间步。）

Figure2.2，如上所述，在10臂老虎机上比较贪婪方法和两个𝜖-贪婪方法（𝜖=0.01和𝜖=0.1）。使用样本平均法计算行动的估计价值（默认初始值为0），第一张图显示随着经验增加，预期奖励的增加。贪婪方法咋一开始比其他两种方法稍快一些，然后在较低的水平趋于平稳。贪婪方法每步的奖励仅为1，而在10臂老虎机上，最佳奖励约为1.54。从长远来看，贪婪方法的变现要差很多，因为它经常被困在执行次优操作上。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/figure-2.2.png)
**Figure2.2**：10臂老虎机上𝜖-贪婪方法的平均性能。图片中的数据是在不同臂上运行2000次的均值。所有𝜖-贪婪方法都使用样本平均法来计算行动的估计价值。

第二张图显示在2000次重复运行中，贪婪方法找到最优动作的仅占运行总数的三分之一，在另外三分之二次运行中，最优动作初始样本的表现令人失望，之后再也没有回到这个动作上（译者注：贪婪行动只会利用已知知识选择最优解，一旦最优行动初始样本的奖励并不是所有行动中最优的，那么在贪婪方法下，该最优行动将永远不会被选择）。𝜖-贪婪方法表现的更好，因为𝜖-贪婪方法会持续不断的探索，以提高识别到最佳行动的机会。𝜖=0.1方法探索的概率会大一些，因此通常能更早的找到最优行动，但选择最优行动的比例从未超过91%。𝜖=0.01改进的速度稍慢，但最终在图中性能指标上的表现优于𝜖=0.1方法。（译者注：这句话没看懂，图中显示𝜖=0.1的性能高于𝜖=0.01，我在这里贴出原文，请读者帮忙看一下是不是我的理解出了问题，The 𝜖=0.01 method improved more slowly, but eventually would perform better than the 𝜖=0.1 method on both performance measures shown in the figure.）你也可以随着时间步的增加，逐步降低𝜖值，以获得最佳效果。

𝜖-贪婪相对于贪婪方法的优势取决于问题本身，举个例子，假设奖励的方差是10而不是1。在奖励噪声较大的情况下，需要更多的探索才能找到最优行动，此时，𝜖-贪婪方法比贪婪方法表现的更好。另一方面，如果奖励的方差为0，那么贪婪方法在尝试一次后就会知道每个动作的真实价值。这种情形下，贪婪方法实际的表现会更好，因为贪婪方法能很快的找到最优行动，并且永远不会探索。但即使在确定性的情况下，如果我们削弱其他一些假设，探索仍有很大的优势。举个例子，拉动老虎机的拉杆获得的奖励不再是平稳的，即行动的真实价值会随着时间的变化而变化。这种情况下仍然需要探索，以确保其中一个非贪婪行动没有变的比贪婪行动更好。在接下来的几章中我们会看到，非平稳性是强化学习中最常见的情况。即使问题本身是平稳的、确定的，学习者仍然会面临一系列随着学习进行和代理决策策略变化而不断变化的情况。强化学习需要在探索和利用之间取得平衡。

_练习2.2：老虎机示例_ 考虑将𝜖-贪婪方法应用于4臂老虎机问题，用数字1、2、3、4表示老虎机的4个拉杆，初始化4个行动的估计价值为0，即对所有的行动a来说$`Q_1(a)=0`$，并用样本均值法计算每个行动的估计价值。假设操作和奖励的顺序是：$`A_1=1,R_1=-1,A_2=2,R_2=1,A_3=2,R_3=-2,A_4=2,R_4=2,A_5=3,R_5=0`$，在其中的某些时间步中𝜖概率可能生效使的行动被随机选择。请问在上述五个行动中，那些时间步中一定采取了随机选择，那些时间步中可能采取了随机选择。

_答案_：$`A_1,A_2,A_3`$可能采取了随机选择，$`A_4,A_5`$一定采取了随机选择。

_练习2.3：_ 在图2.2所示的比较中，从长期来看，哪种方法在累计奖励和选择最佳动作方面表现的更好？会好到什么程度？定量地表达你的答案。
_答案_：贪婪方法不能保证一定找到最优解，故排除掉贪婪方法，对于𝜖-贪婪方法，每此行动选择的最优行动的概率是$`1-𝜖+\frac{𝜖}{n}`$其中n为可选择的行动总数，套用公式，𝜖=0.01时能够挑选到最优动作的概率是99.1%，𝜖=0.1时能够挑选到最优动作的概率是91%，故𝜖=0.01优于𝜖=0.1。从长期来看，探索的概率越小，挑选到最优动作的概率越高。

## 2.4 增量实现
目前为止，我们讨论的行动价值方法都将行动的估计价值估计为观察到的奖励的样本平均值。现在我们去讨论如何计算这些均值，特别时在恒定内存和恒定时间步长内计算。
为简化符号，我们集中探讨一个动作，$`R_i`$表示第i次行动后获得的奖励，$`Q_n`$表示行动被选择n-1次后的估计价值，我们可以简单的写成

$$Q_n\dot{=}\frac{R_1+R_2+···+R_{n-1}}{n-1}$$

显而易见的实现方法是记录所有的奖励，需要估计值时将记录的奖励加和计算。但是，如果这样做，随着获得的奖励增多，内存和计算的需求也会随之增长。每新增一个奖励都需要额外的内存来存储，并重新计算分子中的总和。

你可能会怀疑，把每次奖励值记录下来并重新计算奖励总和是否真的有必要。你的怀疑是对的，我们很容易想到用增量公式来更新平均值。相比于全量公式，增量公式处理每个新奖励时需要的计算量小且恒定。给予Q_n和第n步的奖励R_n，新的奖励平均值可以被计算为
$$Q_{n+1} = \frac{1}{n}\sum\limits_{i=1}^{n}R_i= \frac{1}{n}(R_n+\sum\limits_{i=1}^{n-1}R_i)= \frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum\limits_{i=1}^{n-1}R_i)=\frac{1}{n}(R_n+(n-1)Q_n)=\frac{1}{n}(R_n+nQ_n-Q_n)=Q_n+\frac{1}{n}[R_n-Q_n] （2.3）$$

在n=1时，$`Q_2=R_1`$，此实现方法只需要记录$`Q_n`$和n，每此获得奖励只需要很小的计算量。

公式2.3的更新规则是本书中最常出现的更新规则，一般形式为

$$新估计值 \leftarrow 旧估计值+ 步长[目标-旧估计值]$$

表达式$`[目标-旧估计值]`$是估计的误差。通过向‘目标’迈进以减小误差。目标指示了一个理想的前进方向，尽管目标可能带有噪声。在老虎机问题中目标是第n次获得的奖励。

我们注意到，在公式2.3增量方法中使用的步长参数会随着时间步长的变化而变化。处理动作a第n次奖励时，增量方法使用的步长参数为$`\frac{1}{n}`$。本书中我们用$`\alpha`$或$`\alpha_t(a)`$来表示步长参数。

以下伪代码使用了样本平均法，$`\epsilon`$-贪婪方法和增量公式表示老虎机问题的算法，函数bandit(a)返回动作a相应的奖励。
![image](https://github.com/zhangyi11/Reinforcement-Learning-An-Introduction-/blob/main/images/A%20simple%20bandit%20algorithm.jpg)

## 2.5 追踪非平稳问题
到目前为止讨论的求平均值方法都是针对稳态老虎机问题，即奖励不随时间的变化而变化。如前所述，我们接触的大多数强化学习问题都是非稳态的。在非稳态的强化学习问题中，短期内获得奖励的权重高于长期的奖励权重是有意义的。最常用的方法就是使用一个固定的步长参数。例如在增量公式2.3中，用前n-1次奖励更新平均值$`Q_n`$的公式可以调整为

$$Q_{n+1}\dot{=}Q_n+\alpha[R_n-Q_n]$$

其中$`\alpha\in(0,1]`$。公式表明$`Q_{n+1}`$是初始估计值$`Q_1`$和过去奖励和的加权平均。

$$ Q_{n+1} = Q_n+\alpha[R_n-Q_n] = \alpha R_n+(1-\alpha)Q_n = \alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]=(1-\alpha)^nQ_1+\sum_{i=1}^{n}\alpha(1-\alpha)^{n-1}R_i$$

$$ 其中，(1-\alpha)^n + \sum_{i=1}^{n}\alpha(1-\alpha)^{n-1} = 1，即权重之和为1 $$

权重$`\alpha(1-\alpha)^{n-1}`$取决于$`R_i`$之间有多少奖励，$`1-\alpha`$的值小于1，介于两个奖励之间的奖励数量越多，$`R_i`$的权重就越小。实际上，权重根据$`1-\alpha`$上的指数呈指数衰减（如果$`1-\alpha=0`$，那么所有的权重都放在最后的奖励$`R_n`$上，因为$`0^0=1`$）。

$`\alpha_n(a)`$表示用于处理第n次选择行动a获得的奖励参数。如前所述，样本平均法的步长参数$`\alpha_n(a)=\frac{1}{n}`$，大数定律保证了该方法可以收敛到真实的行动价值。但是无法保证序列$`{\alpha_n(a)}`$的所有选择都能收敛。随机近似理论中的著名结论确保了与概率1收敛所需要的条件

$$\sum_{n=1}^{\infty}\alpha_n(a)=\infty \ \ 和\ \  \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty \ \ (2.7)\ $$

第一个条件是必须得，以保证在步数足够大的情况下，可以克服任何初始条件和随机波动。第二个条件确保步长最终会变得足够小，以保证收敛性。

对于样本平均方法论来说,$`\alpha_n(a)=\frac{1}{n}`$满足2.7中的两个条件，但恒定步长参数如$`\alpha_n(a)=\alpha`$则不满足2.7中的第二个条件，这表明估计值不会完全的收敛，而是根据最近收到的奖励变化，如前所述，在非平稳强化学习中恒定的步长参数是最常见的。此外，满足条件(2.7)的步长参数收敛速度一般很慢，或者需要进行大量的调整才能获得令人满意的收敛速度。总的来说，满足2.7收敛条件的步长参数序列在理论工作中经常使用，但在实际中却很少使用。

_练习2.4_ 如果步长参数$`\alpha_n`$不是恒定的，估计价值$`Q_n`$是先前获得奖励的加权平均值，请仿照公式2.6，给出新的公式
_答案_  

$$ Q_{n+1} = Q_n + \alpha_n[R_n-Q_n]=\alpha_nR_n+(1-\alpha_n)Q_n=\alpha_nR_n+(1-\alpha_n)[\alpha_{n-1}R_{n-1}+(1-\alpha_{n-1})Q_{n-1}]=(1-\alpha_1)^nQ_1+\sum_{i=1}^n\alpha_iR_i\prod_{j=i+1}^n(1-\alpha_j) $$

_练习2.5_ （编程）请设计一个实验，以展现样本平均法在非稳态问题时遇到的困难。例如，使用10臂老虎机，最初老虎机每个臂的$`q_*(a)`$相等，然后每个臂的真实奖励随机独立改变（例如，每一步中，给所有的$`q_*(a)`$添加一个均值为0，方差为0.01的增量。）使用样本平均法、增量公式、以及恒定的步长参数如$`\alpha=0.1`$和$`\epsilon`$-贪婪方法（$`\epsilon`$=0.1），运行10000步。

_答案_
```
import numpy as np

class Ten_Armed_Testbed:
    def __init__(self,num_arms=10,epsilon=0.1,alpha=0.1):
        self.arms = num_arms
        self.epsilon = epsilon
        self.true_values = [0] * num_arms   # 行动的真实价值，初始值为0
        self.estimate_values = [0] *num_arms  # 行动的估计价值
        self.count = [0] * num_arms  # 记录每个行动的选择次数（仅样本平均法，即非固定步长时使用）
        self.alpha = alpha

    def get_action(self):
        if np.random.random() > self.epsilon:
            return np.argmax(self.estimate_values)
        else: return np.random.choice(self.arms)

    def step(self,action,constant_parameter=False):
        reward = np.random.normal(self.true_values[action],1)  # 假定每个行动的实际奖励值服从均值为q*(a)，方差为1的正态分布。
        self.true_values = [i+np.random.normal(0,0.01) for i in self.true_values]  # 每一个行动后，给给所有的q*(a)添加一个均值为0，方差为0.01的随机增量
        self.count[action] += 1
        if constant_parameter:
            self.estimate_values[action] += self.alpha*(reward - self.estimate_values[action])  # 增量公式，固定步长，计算行动的估计价值
        else:
            if self.count[action] != 0:
                self.estimate_values[action] += 1/self.count[action]*(reward - self.estimate_values[action])



def run_experiment(bandit, steps=100000, trials=10):  # 步数在1w时，样本平均法有时会优于固定参数，但是步数在10w时，固定参数总是优于样本平均法
    for _ in range(trials):
        for constant_step_size in [False, True]:
            for _ in range(steps):
                action = bandit.get_action()
                bandit.step(action, constant_step_size)
            error = np.abs(np.mean(np.array(bandit.true_values) - np.array(bandit.estimate_values)))
            print(f"{constant_step_size = },{error = }")
        print("-"*10)
        # print(f"{bandit.count = }")

if __name__ == "__main__":
    bandit = Ten_Armed_Testbed()
    run_experiment(bandit)

#(译者注：上述代码是译者自己写的，可能存在问题，仅供参考哈）
```









