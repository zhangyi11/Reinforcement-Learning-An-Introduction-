# Chapter2 多臂老虎机
强化学习与其他类型学习最重要的区别在于，强化学习利用训练信息来评估所采取的行动，而不是给出正确的行动来指导。这就产生了积极探索的需求和搜索好行为的需求。纯粹的评估性反馈能指出行动的好坏程度，但无法表明该行动是最好或最差的行动。另一方面，纯粹的指导性反馈表明了要采取的正确行动，独立于实际采取的行动。指导性反馈是监督学习的基础，涵盖了模式分类、系统识别、人工神经网络的大部分内容。这两种反馈是截然不同的，评估反馈完全依赖于所采取的行动，而指导性反馈则独立于所采取的行动。

在本章中，我们在一个简化的环境中研究强化学习的评估方面，这个环境只涉及在单一情境中的学习。这种非关联性的环境是大多数之前关于评估反馈的研究所采用的，它避免了完整强化学习问题的复杂性 **（译者注：之前的研究指的是在强化学习领域中，以非关联性环境为基础做的研究工作，这些研究主要关注评估反馈如何影响学习过程，而不涉及在不同情境中选择不同最佳行动的复杂性）**。研究这种情况使我们能够更清楚地理解评估反馈与指导性反馈之间的区别，同时也能看到它们如何结合使用。

我们探讨的特定非关联性，评估反馈问题是k臂老虎机问题的简化版。我们用这个问题介绍一些学习的基本方法，并在之后的章节逐步扩展这些方法，应用于完整的强化学习问题。在本章的结尾，我们将进一步接近完整的强化学习，讨论当老虎机问题变成关联性问题时会发生什么，也就是说，当最佳行动取决于具体情境时会如何。


##  2.1 k臂老虎机问题
考虑如下学习问题，你需要不断地在k个不同的选项中做出选择，在每次选择后都会得到数值奖励，该数值奖励服从稳态概率分布，该分布取决与你的选择 **（译者注：稳态概率分布是指在一个随机过程或马尔可夫链中，随着时间推移，系统状态的概率分布趋于稳定，不再随时间变化。）** 你的目标是在某个时间段内最大化预期总奖励，例如，1000个行动选择或1000个时间步长。

这就是k臂老虎机问题的原始形式，因类比于老虎机而得名，只不过k臂老虎机有k个拉杆而不是一个。每次选择就像是拉动k臂老虎机的一个拉杆，奖励就是击中头奖的回报。在重复的拉动中，你将集中拉动k个拉杆之中的一个，以获得最大的奖励。

在我们的k臂老虎机问题中，每个拉杆被选择时，都会有一个期望或平均奖励，我们将平均奖励称为该行动的价值。我们用A<sub>t</sub>表示在时间步t做的选择，选择A<sub>t</sub>的奖励用R<sub>t</sub>表示。任意操作a的价值用$`q_{\ast}(a)`$表示，$`q_{\ast}(a)`$是行动a的预期奖励：
$q_{\ast}(a) \dot{=} \mathbb{E}[R_t \mid A_t = a]$

如何你知道每个行动的价值，那么解决k臂老虎机问题将会变的非常容易：做选择时，总是拉动具有最高价值的拉杆。假设你不确定每个行动的价值，但你知道每个行动的估计价值，我们将时间步t行动a的估计价值记为Q<sub>t</sub>(a)。我们希望Q<sub>t</sub>(a)无限接近$`q_{\ast}(a)`$。

如果你持续的更新每个行动的估计价值，那么在任意时间步做选择时，至少有一个行动的估计价值是最大的。我们称之为贪婪行动 **（译者注：贪婪行动，即每次做选择时都选择估计价值最大的行动为实际行动）** 当您选择估计值最大的行动作为实际行动的时候，我们说你正在**利用**当前行动价值的知识。如果你选择一个非贪婪行动，那我们说你正在**探索**，探索可以帮助你改进非贪婪行动的估计价值。在某一时间步，为了最大化预期的奖励，利用（即选择贪婪行动）是正确的做法，但从长远来看，探索可能会产生更大的总奖励。

举个例子，假设贪婪行动的价值是确定的，而与贪婪行动的价值相比，其他几个行动的估计价值相当或相近。但是非贪婪行动伴随着很大的不确定性。这种不确定性意味着，尽管贪婪行动的价值看似最优，但实际上至少有一个非贪婪行动的价值比贪婪行动的价值高，只是我们无法确切知道是哪一个非贪婪行动。如果你在未来有很多时间步做选择，那么最好探索非贪婪行动，发现非贪婪行动中哪一个行动的价值高于贪婪行动。

从短期来看，探索带来的奖励较低，但从长期来看，探索带来的奖励更高，因为一旦你发现了比贪婪行动更好的行动，你就可以利用它很多次。由于无法在一次选择中兼顾探索和利用，因此探索和利用之间存在“冲突”。

做选择时，探索和利用哪个更好，取决于估计价值的精确度、不确定性以及剩余步数等复杂因素。对于k臂老虎机等相关特定问题的数学模型，有许多复杂的方法平衡探索与利用。然而，这些方法中的大多数都对平稳性和先验知识做出了强有力的假设，这些假设在大多数应用中、以及在我们后续章节的完整强化学习问题中要么不成立，要么无法验证。当理论假设不适用时，这些方法所提供的最优性或有界损失对我们没有实质性的帮助。

本书不会用复杂的方式详细讨论如何平衡探索和利用，我们只关心如何在整体上平衡探索和利用。本章会介绍几种简单的平衡方法来解决k臂老虎机问题，平衡探索和利用的方法比总是利用的方法效果要好得多。平衡探索和利用是强化学习中独具特色的挑战，k臂老虎机能够清晰的展示这一点。

## 2.2 行动价值方法
首先，我们研究行动的估计价值函数，并用行动的估计价值做出决策。回想一下，行动的真实价值是选择该行动时会得到的平均奖励。因此，我们自然的会想到用行动之前获得奖励的平均值作为该行动的估计价值:
$$Q_t(a)\dot{=}\frac{时间步t之前行动a获得奖励的总和}{时间步t之前行动a被选择的次数}=\sum\limits_{i=1}^{t-1}R_i \cdot \mathbf{1}_{\{A_i = a\}}$$
公式中$`1_{predicate}`$指，当predicate为真时$`1_{predicate}`$的值为1，当predicate为假时，$`1_{predicate}`$的值为0。如果分母为0，我们将$`Q_t(a)`$定义为某个默认值，例如0。当分母趋于无穷时，根据大数定律$`Q_t(a)`$收敛于$`q_*(a)`$，我们此公式来估计行动的价值，由于每个估计值都是样本相关奖励的平均值，故称此方法为样本平均法。样本平均法只是估计行动价值的一种方法，不一定是最好的方法。现在，我们来探讨如何使用样本平均法计算出的估计值来选择动作。

最简单的选择动作规则就是选择具有最高估计价值的行动，即上一节定义的贪婪操作。如果有多个贪婪的行动（译者注：有多个相等的估计价值），那么就会以任意方式挑选一个，也许是随机挑选。我们将贪婪的行动选择方法写成
$$A_t\dot{=}\underset{a}{argmax}$$



