# Chapter2 多臂老虎机
强化学习与其他类型学习最重要的区别在于，强化学习利用训练信息来评估所采取的行动，而不是给出正确的行动来指导。这就产生了积极探索的需求和搜索好行为的需求。纯粹的评估性反馈能指出行动的好坏程度，但无法表明该行动是最好或最差的行动。另一方面，纯粹的指导性反馈表明了要采取的正确行动，独立于实际采取的行动。指导性反馈是监督学习的基础，涵盖了模式分类、系统识别、人工神经网络的大部分内容。这两种反馈是截然不同的，评估反馈完全依赖于所采取的行动，而指导性反馈则独立于所采取的行动。

在本章中，我们在一个简化的环境中研究强化学习的评估方面，这个环境只涉及在单一情境中的学习。这种非关联性的环境是大多数之前关于评估反馈的研究所采用的，它避免了完整强化学习问题的复杂性 **（译者注：之前的研究指的是在强化学习领域中，以非关联性环境为基础做的研究工作，这些研究主要关注评估反馈如何影响学习过程，而不涉及在不同情境中选择不同最佳行动的复杂性）**。研究这种情况使我们能够更清楚地理解评估反馈与指导性反馈之间的区别，同时也能看到它们如何结合使用。

我们探讨的特定非关联性，评估反馈问题使k臂老虎机问题的简化版。我们用这个问题介绍一些学习的基本方法，并在之后的章节逐步扩展这些方法，应用于完整的强化学习问题。在本章的结尾，我们将进一步接近完整的强化学习，讨论当老虎机问题变成关联性问题时会发生什么，也就是说，当最佳行动取决于具体情境时会如何。


##  2.1 k臂老虎机问题
你先在需要不断的在k个不同的选项和行动中做出选择，在每次选择后都会得到数值奖励，该数值奖励服从稳态概率分布，该分布取决与你的选择 **（译者注：稳态概率分布是指在一个随机过程或马尔可夫链中，随着时间推移，系统状态的概率分布趋于稳定，不再随时间变化。）** 你的目标是在某个时间段内最大化预期总奖励，例如，1000个行动选择或1000个时间步长。

这就是k臂老虎机问题的原始形式，因类比于老虎机（或称为“独臂强盗”）而得名，只不过它有k个拉杆而不是一个。每次行动选择就像是拉动其中一个老虎机的拉杆，而奖励就是击中头奖的回报。在重复的拉动老虎机，你将集中拉动k个拉杆之中的一个，以获得最大的奖励。

在我们的k臂老虎机问题中，每个拉杆被选择时，都会有一个期望或平均奖励，我们将平均奖励称为该动作的价值。我们用A<sub>t</sub>表示在t时刻做的选择，选择A<sub>t</sub>的奖励用R<sub>t</sub>表示。任意操作a的价值用q<sub>*</sun>(a)表示，q<sub>*</sub>(a)是行动a的预期奖励：
$q^{\ast}(a) \dot{=} \mathbb{E}[R_t \mid A_t = a]$


